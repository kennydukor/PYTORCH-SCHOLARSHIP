{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"PYTORCH CHALLENGE.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"TPU"},"cells":[{"metadata":{"id":"ZRxoELPreiQz","colab_type":"code","outputId":"4ddfbab7-5280-421a-cb81-91b34182b9b1","executionInfo":{"status":"ok","timestamp":1543338742757,"user_tz":-60,"elapsed":90248,"user":{"displayName":"Kenechi Dukor","photoUrl":"","userId":"13097459652657710283"}},"colab":{"base_uri":"https://localhost:8080/","height":139}},"cell_type":"code","source":["!pip install torch"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Collecting torch\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/49/0e/e382bcf1a6ae8225f50b99cc26effa2d4cc6d66975ccf3fa9590efcbedce/torch-0.4.1-cp36-cp36m-manylinux1_x86_64.whl (519.5MB)\n","\u001b[K    100% |████████████████████████████████| 519.5MB 29kB/s \n","tcmalloc: large alloc 1073750016 bytes == 0x5a12e000 @  0x7fc0b05e52a4 0x591a07 0x5b5d56 0x502e9a 0x506859 0x502209 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x507641 0x502209 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x507641 0x504c28 0x502540 0x502f3d 0x507641\n","\u001b[?25hInstalling collected packages: torch\n","Successfully installed torch-0.4.1\n"],"name":"stdout"}]},{"metadata":{"id":"tHPwJn7rgRfQ","colab_type":"text"},"cell_type":"markdown","source":["Lets build a Simple Neural Network with pytorch"]},{"metadata":{"id":"N8_kormuSNFP","colab_type":"code","colab":{}},"cell_type":"code","source":["# http://pytorch.org/\n","from os.path import exists\n","from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n","platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n","cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n","accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n","\n","!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision"],"execution_count":0,"outputs":[]},{"metadata":{"id":"E7PRTA5Rf1LB","colab_type":"code","colab":{}},"cell_type":"code","source":["#import pytorch\n","import torch"],"execution_count":0,"outputs":[]},{"metadata":{"id":"y8M51Kk_gbIt","colab_type":"code","colab":{}},"cell_type":"code","source":["#lets create the activation function (sigmoid activation function)\n","def activation (x):\n","  #note: the input x will be a tensor\n","  return 1/(1 + torch.exp(-x))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"t48WflFSg487","colab_type":"code","colab":{}},"cell_type":"code","source":["##Lets generate random data\n","torch.manual_seed(7) #set the random seed so that things are predictable\n","\n","##5 random normal features\n","features = torch.randn((1, 5))\n","\n","#true  weights for our data that are random\n","weights = torch.randn_like(features)\n","\n","#true bias term\n","bias = torch.randn((1,1))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Z8fnONAzhG-r","colab_type":"code","outputId":"fbad02a4-48b7-4f98-a050-0a1af7d99632","executionInfo":{"status":"ok","timestamp":1543338754427,"user_tz":-60,"elapsed":101884,"user":{"displayName":"Kenechi Dukor","photoUrl":"","userId":"13097459652657710283"}},"colab":{"base_uri":"https://localhost:8080/","height":88}},"cell_type":"code","source":["print (features)\n","print (weights)\n","print (bias)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["tensor([[-0.1468,  0.7861,  0.9468, -1.1143,  1.6908]])\n","tensor([[-0.8948, -0.3556,  1.2324,  0.1382, -1.6822]])\n","tensor([[0.3177]])\n"],"name":"stdout"}]},{"metadata":{"id":"UZkP5EatokDo","colab_type":"code","outputId":"304de671-5cbf-484f-c488-adb65fba536a","executionInfo":{"status":"ok","timestamp":1543338754429,"user_tz":-60,"elapsed":101855,"user":{"displayName":"Kenechi Dukor","photoUrl":"","userId":"13097459652657710283"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"cell_type":"code","source":["# now, to form the simple neural network,\n","\n","y = activation(torch.sum(features * weights) + bias )\n","print(y)\n","#or\n","y = activation((features * weights).sum() + bias )\n","print(y)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["tensor([[0.1595]])\n","tensor([[0.1595]])\n"],"name":"stdout"}]},{"metadata":{"id":"8Q1tr478nUVt","colab_type":"code","outputId":"38fd54c8-722b-46dc-a821-9fa3ba7fd790","executionInfo":{"status":"ok","timestamp":1543338754431,"user_tz":-60,"elapsed":101830,"user":{"displayName":"Kenechi Dukor","photoUrl":"","userId":"13097459652657710283"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["# we can also acheive this using matrix multiplication. but we have to resize the weight to prevent error\n","# you can use weights.resize_(a, b), weight.reshape(a,b), and weight.view(a,b). weight. view is advised bbecause the result does not tamper with the data\n","weights = weights.view(5, 1)\n","\n","y = activation(torch.mm(features, weights) + bias) #or torch.matmul\n","print(y)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["tensor([[0.1595]])\n"],"name":"stdout"}]},{"metadata":{"id":"bGU8QgDjAoD1","colab_type":"code","colab":{}},"cell_type":"code","source":["# Generate some data for a multi layer nn\n","torch.manual_seed(7)\n","\n","#random features\n","features = torch.randn((1, 3))\n","\n","#define the size of each layer\n","n_input = features.shape[1] # no of input unit = no of input features\n","\n","n_hidden = 2\n","\n","n_output = 1\n","\n","#wiehgt for input to hidden layer\n","W1 = torch.randn((n_input, n_hidden))\n","\n","#weight of hidden layer\n","W2 = torch.randn((n_hidden, n_output))\n","\n","#bias for hidden and output layer\n","\n","B1 = torch.randn((1, n_hidden))\n","\n","B2 = torch.randn((1, n_output))\n","\n","\n","\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"o1RO0wBrS2Ui","colab_type":"code","outputId":"28428024-0615-4abc-dcc5-fe419cd357b4","executionInfo":{"status":"ok","timestamp":1543338754439,"user_tz":-60,"elapsed":101806,"user":{"displayName":"Kenechi Dukor","photoUrl":"","userId":"13097459652657710283"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["layer_1 = activation(torch.mm(features, W1) + B1)\n","\n","layer_2 = torch.mm(layer_1, W2) + B2\n","\n","y = activation(layer_2)\n","\n","y"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[0.3171]])"]},"metadata":{"tags":[]},"execution_count":10}]},{"metadata":{"id":"64iMBaOVUYyM","colab_type":"code","colab":{}},"cell_type":"code","source":["#converting btw numpy array and torch tensors\n","\n","import numpy as np\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Ms89xWQGUprf","colab_type":"code","outputId":"a46a26eb-148a-44b0-f149-e6522b666eea","executionInfo":{"status":"ok","timestamp":1543338754445,"user_tz":-60,"elapsed":101780,"user":{"displayName":"Kenechi Dukor","photoUrl":"","userId":"13097459652657710283"}},"colab":{"base_uri":"https://localhost:8080/","height":85}},"cell_type":"code","source":["a = np.random.rand(4, 3)\n","\n","a"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0.85372244, 0.29591765, 0.24780362],\n","       [0.45034136, 0.46591312, 0.84390031],\n","       [0.74963355, 0.91989858, 0.25575832],\n","       [0.42662328, 0.71252705, 0.90332782]])"]},"metadata":{"tags":[]},"execution_count":12}]},{"metadata":{"id":"OmPSJxCUUx7Q","colab_type":"code","outputId":"44e797ec-dd4e-489d-a1bd-8fa917bcb96e","executionInfo":{"status":"ok","timestamp":1543338754447,"user_tz":-60,"elapsed":101755,"user":{"displayName":"Kenechi Dukor","photoUrl":"","userId":"13097459652657710283"}},"colab":{"base_uri":"https://localhost:8080/","height":85}},"cell_type":"code","source":["b = torch.from_numpy(a)\n","b"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[0.8537, 0.2959, 0.2478],\n","        [0.4503, 0.4659, 0.8439],\n","        [0.7496, 0.9199, 0.2558],\n","        [0.4266, 0.7125, 0.9033]], dtype=torch.float64)"]},"metadata":{"tags":[]},"execution_count":13}]},{"metadata":{"id":"5SZT4pPcU7cs","colab_type":"code","outputId":"f196cb10-9d10-41d1-9295-f9f204aa3b05","executionInfo":{"status":"ok","timestamp":1543338754450,"user_tz":-60,"elapsed":101741,"user":{"displayName":"Kenechi Dukor","photoUrl":"","userId":"13097459652657710283"}},"colab":{"base_uri":"https://localhost:8080/","height":85}},"cell_type":"code","source":["b.numpy()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0.85372244, 0.29591765, 0.24780362],\n","       [0.45034136, 0.46591312, 0.84390031],\n","       [0.74963355, 0.91989858, 0.25575832],\n","       [0.42662328, 0.71252705, 0.90332782]])"]},"metadata":{"tags":[]},"execution_count":14}]},{"metadata":{"id":"5wtp5KQ1VGRG","colab_type":"code","outputId":"5342bdfa-0061-4855-e432-1766cfe5e1b8","executionInfo":{"status":"ok","timestamp":1543338754450,"user_tz":-60,"elapsed":101723,"user":{"displayName":"Kenechi Dukor","photoUrl":"","userId":"13097459652657710283"}},"colab":{"base_uri":"https://localhost:8080/","height":85}},"cell_type":"code","source":["#lets multiply the torch tensor by 2\n","b.mul_(2)\n","b"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1.7074, 0.5918, 0.4956],\n","        [0.9007, 0.9318, 1.6878],\n","        [1.4993, 1.8398, 0.5115],\n","        [0.8532, 1.4251, 1.8067]], dtype=torch.float64)"]},"metadata":{"tags":[]},"execution_count":15}]},{"metadata":{"id":"83XXeWieVUbH","colab_type":"code","outputId":"ac3129a6-68f3-4651-8633-1c5e37d25e13","executionInfo":{"status":"ok","timestamp":1543338754454,"user_tz":-60,"elapsed":101709,"user":{"displayName":"Kenechi Dukor","photoUrl":"","userId":"13097459652657710283"}},"colab":{"base_uri":"https://localhost:8080/","height":85}},"cell_type":"code","source":["#array a will also be affected. ie memory is shared btw numpy array and torch tensor\n","a"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[1.70744487, 0.59183529, 0.49560725],\n","       [0.90068271, 0.93182624, 1.68780062],\n","       [1.49926709, 1.83979717, 0.51151664],\n","       [0.85324656, 1.4250541 , 1.80665565]])"]},"metadata":{"tags":[]},"execution_count":16}]},{"metadata":{"id":"oFrxgI5KV0IN","colab_type":"text"},"cell_type":"markdown","source":["# Lets do some real neural network with MNIST **DATASET**"]},{"metadata":{"id":"aWGISmFfVyXU","colab_type":"code","outputId":"d2644beb-fff0-46ec-8afb-b54c9be7d0a2","executionInfo":{"status":"ok","timestamp":1543338758759,"user_tz":-60,"elapsed":105999,"user":{"displayName":"Kenechi Dukor","photoUrl":"","userId":"13097459652657710283"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"cell_type":"code","source":["!pip install helper\n","%matplotlib inline\n","%config InlineBackend.figure_format = 'retina'\n","import helper\n","import matplotlib.pyplot as plt"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Collecting helper\n","  Downloading https://files.pythonhosted.org/packages/be/27/80bdb3e3bd9808db34ef38b332e984ba955a09d896231ef2ca62564cb6f9/helper-2.4.2-py2.py3-none-any.whl\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from helper) (3.13)\n","Installing collected packages: helper\n","Successfully installed helper-2.4.2\n"],"name":"stdout"}]},{"metadata":{"id":"DDhXHHNPWIX_","colab_type":"code","outputId":"c55f2641-5c01-42b0-d2c8-88a1b7384a4e","executionInfo":{"status":"ok","timestamp":1543338764355,"user_tz":-60,"elapsed":111578,"user":{"displayName":"Kenechi Dukor","photoUrl":"","userId":"13097459652657710283"}},"colab":{"base_uri":"https://localhost:8080/","height":224}},"cell_type":"code","source":["# we will be using the mnist dataset. I think the dataset is in pytorch\n","#this cell downloads and runs the dataset\n","!pip install torchvision\n","\n","#!pip install torchvision\n","import torch\n","from torchvision import datasets, transforms\n","import torchvision.transforms as transforms\n","\n","#define a tranform to normiailze the data\n","# transform = transforms.Compose([tranforms.ToTensor(),\n","#                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n","#                                ])\n","\n","transform = transforms.Compose(\n","    [transforms.ToTensor(),\n","     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n","\n","# download and load the mnist dataset\n","trainset = datasets.MNIST('MNIST_data/', download=True, train=True, transform=transform)\n","trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.2.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.11.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.14.6)\n","Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (5.3.0)\n","Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchvision) (0.4.1)\n","Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n","Processing...\n","Done!\n"],"name":"stdout"}]},{"metadata":{"id":"PLVVOq-o-eA7","colab_type":"text"},"cell_type":"markdown","source":["**Train loader allows us iterate throught the MNIST dataset**"]},{"metadata":{"id":"oOf40_Ch-kRA","colab_type":"code","outputId":"cce9b590-6206-48c6-f0f1-1827f3bbaf8d","executionInfo":{"status":"ok","timestamp":1543338764359,"user_tz":-60,"elapsed":111561,"user":{"displayName":"Kenechi Dukor","photoUrl":"","userId":"13097459652657710283"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"cell_type":"code","source":["#for image, label in trainloader: do somoething\n","dataiter = iter(trainloader)\n","image, labels = dataiter.next()\n","print(type(image))\n","print(image.shape)\n","print(labels.shape)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["<class 'torch.Tensor'>\n","torch.Size([64, 1, 28, 28])\n","torch.Size([64])\n"],"name":"stdout"}]},{"metadata":{"id":"wJKPo2YX7jgj","colab_type":"code","outputId":"fcfc44b6-d4d1-4e46-bbf3-76bcc3c871b5","executionInfo":{"status":"ok","timestamp":1543338765042,"user_tz":-60,"elapsed":112229,"user":{"displayName":"Kenechi Dukor","photoUrl":"","userId":"13097459652657710283"}},"colab":{"base_uri":"https://localhost:8080/","height":282}},"cell_type":"code","source":["plt.imshow(image[1].numpy().squeeze(), cmap='Greys_r')"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<matplotlib.image.AxesImage at 0x7f34bb01a4a8>"]},"metadata":{"tags":[]},"execution_count":20},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAfMAAAHwCAYAAACym4blAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n","text/plain":["<matplotlib.figure.Figure at 0x7f34bb810940>"]},"metadata":{"tags":[],"image/png":{"width":249,"height":248}}}]},{"metadata":{"id":"Eg4AkWHpAloY","colab_type":"text"},"cell_type":"markdown","source":["# **Now, building the network for the digit classifer**"]},{"metadata":{"id":"lHZ59Yw5Zbs9","colab_type":"code","colab":{}},"cell_type":"code","source":["def activation(x):\n","  return 1/(1 + torch.exp(-x))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"eaxXYn9qZrkA","colab_type":"code","outputId":"4d9138d2-3b92-4e6b-ecd1-b97e10e61653","executionInfo":{"status":"ok","timestamp":1543338765055,"user_tz":-60,"elapsed":112226,"user":{"displayName":"Kenechi Dukor","photoUrl":"","userId":"13097459652657710283"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["#lets flatten the input images\n","inputs = image.view(image.shape[0], -1)\n","inputs.shape"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([64, 784])"]},"metadata":{"tags":[]},"execution_count":22}]},{"metadata":{"id":"cmZUE9xo2t1b","colab_type":"code","colab":{}},"cell_type":"code","source":["torch.manual_seed(7)\n","\n","n_input = 784\n","\n","n_hidden = 256\n","\n","n_output = 10\n","\n","W1 = torch.randn((n_input, n_hidden))\n","\n","W2 = torch.randn((n_hidden, n_output))\n","\n","B1 = torch.randn((1, n_hidden))\n","\n","B2 = torch.randn((1, n_output))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"f4cyZS7i3f49","colab_type":"code","outputId":"be5d0ab5-631f-466c-dd06-afbd13ca703e","executionInfo":{"status":"ok","timestamp":1543338765064,"user_tz":-60,"elapsed":112218,"user":{"displayName":"Kenechi Dukor","photoUrl":"","userId":"13097459652657710283"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["h = activation(torch.mm(inputs, W1) + B1)\n","h.shape"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([64, 256])"]},"metadata":{"tags":[]},"execution_count":24}]},{"metadata":{"id":"w7UP67facGW4","colab_type":"code","outputId":"7b1e2db7-ebd1-419b-dd92-3c82d870d272","executionInfo":{"status":"ok","timestamp":1543338765067,"user_tz":-60,"elapsed":112204,"user":{"displayName":"Kenechi Dukor","photoUrl":"","userId":"13097459652657710283"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["h_out = torch.mm(h, W2) + B2\n","print (h_out.shape)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["torch.Size([64, 10])\n"],"name":"stdout"}]},{"metadata":{"id":"aIL0LwlddnUJ","colab_type":"code","outputId":"5ca3bece-04d8-40e8-fc24-23930155a96a","executionInfo":{"status":"ok","timestamp":1543338765069,"user_tz":-60,"elapsed":112189,"user":{"displayName":"Kenechi Dukor","photoUrl":"","userId":"13097459652657710283"}},"colab":{"base_uri":"https://localhost:8080/","height":190}},"cell_type":"code","source":["#now, lets pass it thrugh softmax to get the prob \n","def softmax(x):\n","  return torch.exp(x)/torch.sum(torch.exp(x), dim=1).view(-1, 1) #dim=1 allows us take the sum across the columns\n","#.view(-1, 1) says for -1: for the equivalent row considered, in the 1st column\n","\n","probabilities = softmax(h_out)\n","\n","print(probabilities.shape)\n","\n","print(probabilities.sum(dim=1))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["torch.Size([64, 10])\n","tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000])\n"],"name":"stdout"}]},{"metadata":{"id":"ozraNHGUMXxK","colab_type":"text"},"cell_type":"markdown","source":["Now, lets build the neural network using the init module and torch methods"]},{"metadata":{"id":"Lq6oJcYjMeEx","colab_type":"code","colab":{}},"cell_type":"code","source":["from torch import nn "],"execution_count":0,"outputs":[]},{"metadata":{"id":"Ueoz8k-jMsWT","colab_type":"code","colab":{}},"cell_type":"code","source":["class Network(nn.Module):\n","  def __init__(self):\n","    super().__init__()\n","    \n","    #input to hidden layer linear transformation\n","    self.hidden = nn.Linear(784, 256)\n","    \n","    #Output layer, 10 units - one for each digit \n","    self.output = nn.Linear (256, 10)\n","    \n","    #define sigmoid and softmax function\n","    \n","    self.sigmoid = nn.Sigmoid()\n","    self.softmax = nn.Softmax(dim=1)\n","    \n","  def forward(self, x):\n","    #pass the input tensor thorugh each of our operation\n","    x = self.hidden(x)\n","    x = self.sigmoid(x)\n","    x = self.output(x)\n","    x = self.softmax(x)\n","    return x\n","      "],"execution_count":0,"outputs":[]},{"metadata":{"id":"RKSqdUY1PcYf","colab_type":"code","outputId":"4855a427-0ad5-4f92-f47c-a90eefa3995b","executionInfo":{"status":"ok","timestamp":1543338765355,"user_tz":-60,"elapsed":112459,"user":{"displayName":"Kenechi Dukor","photoUrl":"","userId":"13097459652657710283"}},"colab":{"base_uri":"https://localhost:8080/","height":139}},"cell_type":"code","source":["model = Network()\n","model"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Network(\n","  (hidden): Linear(in_features=784, out_features=256, bias=True)\n","  (output): Linear(in_features=256, out_features=10, bias=True)\n","  (sigmoid): Sigmoid()\n","  (softmax): Softmax()\n",")"]},"metadata":{"tags":[]},"execution_count":29}]},{"metadata":{"id":"jyu5VsFNRf-h","colab_type":"code","outputId":"34917ecf-e390-4cda-9d4a-1a47ef636bf5","executionInfo":{"status":"ok","timestamp":1543338765355,"user_tz":-60,"elapsed":112440,"user":{"displayName":"Kenechi Dukor","photoUrl":"","userId":"13097459652657710283"}},"colab":{"base_uri":"https://localhost:8080/","height":2213}},"cell_type":"code","source":["model.forward(inputs)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[0.1097, 0.1210, 0.1066, 0.1030, 0.0954, 0.0944, 0.0715, 0.1020, 0.0764,\n","         0.1199],\n","        [0.1050, 0.1139, 0.1106, 0.1116, 0.0976, 0.0873, 0.0694, 0.1056, 0.0805,\n","         0.1185],\n","        [0.1055, 0.1174, 0.1014, 0.1046, 0.0959, 0.0986, 0.0723, 0.1098, 0.0801,\n","         0.1144],\n","        [0.1054, 0.1259, 0.1100, 0.1030, 0.0951, 0.0914, 0.0708, 0.1133, 0.0723,\n","         0.1129],\n","        [0.1058, 0.1155, 0.1106, 0.1122, 0.0951, 0.0924, 0.0701, 0.1072, 0.0804,\n","         0.1107],\n","        [0.1020, 0.1205, 0.1063, 0.1099, 0.0977, 0.0966, 0.0677, 0.1097, 0.0791,\n","         0.1106],\n","        [0.1039, 0.1155, 0.1140, 0.1076, 0.0959, 0.0873, 0.0676, 0.1113, 0.0763,\n","         0.1206],\n","        [0.1052, 0.1116, 0.1128, 0.1037, 0.0966, 0.0925, 0.0669, 0.1049, 0.0778,\n","         0.1281],\n","        [0.1047, 0.1153, 0.1050, 0.1100, 0.0978, 0.0908, 0.0740, 0.1037, 0.0789,\n","         0.1198],\n","        [0.0981, 0.1270, 0.1162, 0.1054, 0.0883, 0.0981, 0.0614, 0.1140, 0.0771,\n","         0.1144],\n","        [0.0988, 0.1112, 0.1135, 0.1123, 0.1010, 0.0907, 0.0725, 0.1062, 0.0775,\n","         0.1162],\n","        [0.1052, 0.1232, 0.1103, 0.1071, 0.0922, 0.0946, 0.0696, 0.1078, 0.0770,\n","         0.1130],\n","        [0.1040, 0.1191, 0.1032, 0.1054, 0.0950, 0.0915, 0.0690, 0.1135, 0.0774,\n","         0.1219],\n","        [0.0988, 0.1231, 0.1118, 0.1032, 0.0909, 0.0946, 0.0686, 0.1106, 0.0791,\n","         0.1192],\n","        [0.1022, 0.1138, 0.1090, 0.1131, 0.0925, 0.0916, 0.0741, 0.1063, 0.0789,\n","         0.1185],\n","        [0.1090, 0.1142, 0.1160, 0.1011, 0.0951, 0.0942, 0.0715, 0.1037, 0.0754,\n","         0.1198],\n","        [0.1099, 0.1240, 0.1099, 0.1034, 0.0924, 0.0868, 0.0680, 0.1138, 0.0730,\n","         0.1189],\n","        [0.1084, 0.1151, 0.1257, 0.1097, 0.0935, 0.0853, 0.0680, 0.1005, 0.0790,\n","         0.1148],\n","        [0.1032, 0.1239, 0.1081, 0.0980, 0.0986, 0.0953, 0.0753, 0.1062, 0.0777,\n","         0.1136],\n","        [0.1022, 0.1214, 0.1120, 0.1016, 0.0957, 0.0950, 0.0735, 0.1062, 0.0763,\n","         0.1160],\n","        [0.0999, 0.1241, 0.1061, 0.1052, 0.0979, 0.0914, 0.0748, 0.1024, 0.0823,\n","         0.1159],\n","        [0.1063, 0.1218, 0.1118, 0.0958, 0.0947, 0.0937, 0.0766, 0.1096, 0.0721,\n","         0.1177],\n","        [0.1128, 0.1117, 0.1183, 0.1098, 0.0989, 0.0824, 0.0710, 0.1014, 0.0784,\n","         0.1151],\n","        [0.1004, 0.1226, 0.1100, 0.1104, 0.0967, 0.0916, 0.0704, 0.1088, 0.0818,\n","         0.1072],\n","        [0.1049, 0.1198, 0.1098, 0.1017, 0.0961, 0.0909, 0.0717, 0.1094, 0.0793,\n","         0.1164],\n","        [0.1037, 0.1206, 0.1107, 0.1072, 0.0984, 0.0961, 0.0688, 0.1048, 0.0772,\n","         0.1125],\n","        [0.1060, 0.1265, 0.1066, 0.1070, 0.0952, 0.0958, 0.0696, 0.1111, 0.0724,\n","         0.1099],\n","        [0.1008, 0.1147, 0.1084, 0.1098, 0.0984, 0.0940, 0.0744, 0.1089, 0.0809,\n","         0.1098],\n","        [0.1108, 0.1232, 0.1080, 0.0990, 0.0909, 0.0933, 0.0691, 0.1183, 0.0774,\n","         0.1099],\n","        [0.1058, 0.1167, 0.1058, 0.1063, 0.0974, 0.0928, 0.0677, 0.1149, 0.0763,\n","         0.1163],\n","        [0.1055, 0.1253, 0.1089, 0.1049, 0.0971, 0.0927, 0.0707, 0.1135, 0.0715,\n","         0.1099],\n","        [0.1036, 0.1117, 0.1101, 0.1050, 0.0919, 0.0965, 0.0728, 0.1048, 0.0852,\n","         0.1183],\n","        [0.1039, 0.1154, 0.1180, 0.1073, 0.0956, 0.0904, 0.0661, 0.1032, 0.0829,\n","         0.1171],\n","        [0.1049, 0.1214, 0.1124, 0.1048, 0.0971, 0.0952, 0.0718, 0.1051, 0.0753,\n","         0.1121],\n","        [0.0980, 0.1198, 0.1090, 0.1121, 0.0961, 0.0933, 0.0704, 0.1130, 0.0748,\n","         0.1134],\n","        [0.1096, 0.1226, 0.1132, 0.1028, 0.0963, 0.0898, 0.0649, 0.1100, 0.0768,\n","         0.1140],\n","        [0.1117, 0.1132, 0.1108, 0.1033, 0.0947, 0.0940, 0.0778, 0.1022, 0.0834,\n","         0.1090],\n","        [0.1021, 0.1237, 0.1031, 0.1088, 0.1026, 0.0936, 0.0749, 0.1059, 0.0782,\n","         0.1069],\n","        [0.1077, 0.1144, 0.1124, 0.1044, 0.0919, 0.0924, 0.0733, 0.1068, 0.0790,\n","         0.1176],\n","        [0.1061, 0.1150, 0.1093, 0.1041, 0.0939, 0.0867, 0.0715, 0.1088, 0.0786,\n","         0.1261],\n","        [0.1030, 0.1260, 0.1129, 0.1001, 0.0976, 0.0934, 0.0745, 0.1067, 0.0741,\n","         0.1116],\n","        [0.1066, 0.1156, 0.1084, 0.1063, 0.0944, 0.0913, 0.0679, 0.1091, 0.0793,\n","         0.1212],\n","        [0.1057, 0.1193, 0.1064, 0.1083, 0.0979, 0.0907, 0.0709, 0.1114, 0.0770,\n","         0.1123],\n","        [0.0993, 0.1207, 0.1044, 0.1021, 0.0946, 0.1000, 0.0743, 0.1072, 0.0830,\n","         0.1143],\n","        [0.1073, 0.1223, 0.1005, 0.1052, 0.0983, 0.0934, 0.0720, 0.1087, 0.0754,\n","         0.1169],\n","        [0.1098, 0.1194, 0.1033, 0.1131, 0.0940, 0.0923, 0.0732, 0.1051, 0.0768,\n","         0.1129],\n","        [0.0995, 0.1206, 0.1012, 0.1091, 0.0962, 0.0961, 0.0751, 0.1069, 0.0809,\n","         0.1146],\n","        [0.1008, 0.1219, 0.1005, 0.1001, 0.0997, 0.0968, 0.0743, 0.1127, 0.0805,\n","         0.1127],\n","        [0.1113, 0.1171, 0.1158, 0.1057, 0.0926, 0.0857, 0.0722, 0.1081, 0.0766,\n","         0.1149],\n","        [0.1012, 0.1175, 0.1034, 0.1086, 0.0965, 0.0960, 0.0712, 0.1068, 0.0795,\n","         0.1195],\n","        [0.1045, 0.1286, 0.1060, 0.1063, 0.0953, 0.0907, 0.0704, 0.1128, 0.0713,\n","         0.1140],\n","        [0.1021, 0.1220, 0.1124, 0.1053, 0.0994, 0.0961, 0.0696, 0.1014, 0.0761,\n","         0.1156],\n","        [0.1121, 0.1157, 0.1125, 0.1044, 0.0997, 0.0861, 0.0746, 0.0987, 0.0824,\n","         0.1139],\n","        [0.1045, 0.1158, 0.1111, 0.1060, 0.1005, 0.0885, 0.0691, 0.1067, 0.0766,\n","         0.1211],\n","        [0.1117, 0.1197, 0.1062, 0.1045, 0.0931, 0.0938, 0.0728, 0.1036, 0.0802,\n","         0.1143],\n","        [0.1049, 0.1261, 0.1047, 0.1056, 0.0960, 0.0924, 0.0708, 0.1125, 0.0718,\n","         0.1152],\n","        [0.1074, 0.1171, 0.1106, 0.1033, 0.0917, 0.0928, 0.0702, 0.1072, 0.0753,\n","         0.1244],\n","        [0.1076, 0.1125, 0.1094, 0.1088, 0.1005, 0.0894, 0.0711, 0.1069, 0.0760,\n","         0.1177],\n","        [0.1039, 0.1211, 0.1234, 0.1072, 0.0918, 0.0904, 0.0666, 0.1049, 0.0751,\n","         0.1157],\n","        [0.1056, 0.1204, 0.1081, 0.1089, 0.0965, 0.0954, 0.0703, 0.1103, 0.0722,\n","         0.1122],\n","        [0.1072, 0.1190, 0.1104, 0.1054, 0.0937, 0.0941, 0.0687, 0.1092, 0.0764,\n","         0.1159],\n","        [0.1088, 0.1156, 0.1045, 0.1071, 0.0942, 0.0893, 0.0722, 0.1145, 0.0759,\n","         0.1178],\n","        [0.1028, 0.1264, 0.1098, 0.1075, 0.0993, 0.0906, 0.0702, 0.1119, 0.0721,\n","         0.1093],\n","        [0.1021, 0.1206, 0.1052, 0.1070, 0.0955, 0.0904, 0.0700, 0.1162, 0.0781,\n","         0.1149]], grad_fn=<SoftmaxBackward>)"]},"metadata":{"tags":[]},"execution_count":30}]},{"metadata":{"id":"Mn2Y4i0PTyeQ","colab_type":"text"},"cell_type":"markdown","source":["NOW LETS BUILD A DEEPER NETWORK USING RELU"]},{"metadata":{"id":"GDLbql2RUAsk","colab_type":"code","colab":{}},"cell_type":"code","source":["from torch import nn"],"execution_count":0,"outputs":[]},{"metadata":{"id":"LomRQcEaTTZB","colab_type":"code","colab":{}},"cell_type":"code","source":["class Network(nn.Module):\n","  def __init__(self):\n","    super().__init__()\n","    self.hidden_1 = nn.Linear(784, 128)\n","    self.hidden_2 = nn.Linear(128, 64)\n","    self.output = nn.Linear(64, 10)\n","\n","    self.relu = nn.ReLU()\n","    self.softmax = nn.Softmax()\n","    \n","  def Relu(self, x):\n","    return max(0, x)\n","      \n","  def forward(self, x):\n","    x = self.hidden_1(x)\n","    x = self.relu(x)    \n","    x = self.hidden_2(x)\n","    x = self.relu(x)    \n","    x = self.output(x)\n","    x = self.softmax(x)\n","    return x"],"execution_count":0,"outputs":[]},{"metadata":{"id":"4pXRNO_GWJE9","colab_type":"code","outputId":"25eb706e-2657-43a5-cb40-12a4ffd8f521","executionInfo":{"status":"ok","timestamp":1543338765363,"user_tz":-60,"elapsed":112416,"user":{"displayName":"Kenechi Dukor","photoUrl":"","userId":"13097459652657710283"}},"colab":{"base_uri":"https://localhost:8080/","height":156}},"cell_type":"code","source":["model = Network()\n","model"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Network(\n","  (hidden_1): Linear(in_features=784, out_features=128, bias=True)\n","  (hidden_2): Linear(in_features=128, out_features=64, bias=True)\n","  (output): Linear(in_features=64, out_features=10, bias=True)\n","  (relu): ReLU()\n","  (softmax): Softmax()\n",")"]},"metadata":{"tags":[]},"execution_count":33}]},{"metadata":{"id":"RDHKlGklWlik","colab_type":"code","outputId":"fd1f0930-fa47-448d-849d-c78ecff49b66","executionInfo":{"status":"ok","timestamp":1543338765364,"user_tz":-60,"elapsed":112402,"user":{"displayName":"Kenechi Dukor","photoUrl":"","userId":"13097459652657710283"}},"colab":{"base_uri":"https://localhost:8080/","height":2330}},"cell_type":"code","source":["model(inputs)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["tensor([[0.1072, 0.0892, 0.1022, 0.0980, 0.1069, 0.1019, 0.0856, 0.0914, 0.1099,\n","         0.1077],\n","        [0.1152, 0.0869, 0.0981, 0.0898, 0.1104, 0.1015, 0.0912, 0.0861, 0.1071,\n","         0.1138],\n","        [0.1097, 0.0866, 0.1030, 0.0989, 0.1101, 0.0976, 0.0874, 0.0905, 0.1099,\n","         0.1064],\n","        [0.1072, 0.0872, 0.0934, 0.0930, 0.1068, 0.0976, 0.0900, 0.0935, 0.1155,\n","         0.1160],\n","        [0.1066, 0.0895, 0.0996, 0.0877, 0.1037, 0.0966, 0.0929, 0.0899, 0.1197,\n","         0.1138],\n","        [0.1084, 0.0863, 0.1006, 0.0938, 0.1089, 0.0996, 0.0910, 0.0895, 0.1078,\n","         0.1142],\n","        [0.1053, 0.0920, 0.1012, 0.0924, 0.1039, 0.0999, 0.0910, 0.0851, 0.1151,\n","         0.1141],\n","        [0.1017, 0.0871, 0.0989, 0.0946, 0.1079, 0.1029, 0.0952, 0.0841, 0.1136,\n","         0.1140],\n","        [0.1032, 0.0849, 0.1058, 0.0989, 0.1086, 0.0945, 0.0936, 0.0898, 0.1106,\n","         0.1101],\n","        [0.1116, 0.0942, 0.0966, 0.0910, 0.1055, 0.0969, 0.0831, 0.0921, 0.1115,\n","         0.1174],\n","        [0.1010, 0.0884, 0.1006, 0.0957, 0.1081, 0.1014, 0.0923, 0.0858, 0.1113,\n","         0.1154],\n","        [0.1065, 0.0910, 0.0977, 0.0939, 0.1018, 0.0928, 0.0929, 0.0995, 0.1126,\n","         0.1112],\n","        [0.1092, 0.0862, 0.0898, 0.0933, 0.1124, 0.1006, 0.0883, 0.0970, 0.1065,\n","         0.1168],\n","        [0.1042, 0.0928, 0.0947, 0.0956, 0.0970, 0.0970, 0.0955, 0.0922, 0.1140,\n","         0.1172],\n","        [0.0997, 0.0904, 0.1037, 0.1002, 0.1083, 0.0995, 0.0933, 0.0930, 0.1029,\n","         0.1088],\n","        [0.1106, 0.0943, 0.0960, 0.0933, 0.1131, 0.0940, 0.0844, 0.0961, 0.1063,\n","         0.1119],\n","        [0.1086, 0.0902, 0.0998, 0.0913, 0.1075, 0.1000, 0.0906, 0.0863, 0.1058,\n","         0.1200],\n","        [0.1079, 0.0932, 0.1021, 0.0899, 0.1017, 0.0893, 0.0985, 0.0803, 0.1143,\n","         0.1228],\n","        [0.1086, 0.0841, 0.0939, 0.0954, 0.1151, 0.1007, 0.0875, 0.0948, 0.1062,\n","         0.1139],\n","        [0.1089, 0.0940, 0.0977, 0.0952, 0.1125, 0.1011, 0.0849, 0.0908, 0.1053,\n","         0.1096],\n","        [0.1010, 0.0882, 0.1013, 0.0976, 0.1075, 0.0951, 0.0911, 0.0929, 0.1094,\n","         0.1159],\n","        [0.1093, 0.0866, 0.0962, 0.0981, 0.1152, 0.0995, 0.0861, 0.0936, 0.1037,\n","         0.1117],\n","        [0.1129, 0.0861, 0.1016, 0.0885, 0.1078, 0.1016, 0.0833, 0.0860, 0.1179,\n","         0.1143],\n","        [0.1070, 0.0879, 0.0919, 0.0925, 0.0992, 0.1010, 0.0990, 0.0885, 0.1197,\n","         0.1134],\n","        [0.1061, 0.0912, 0.0958, 0.0926, 0.1077, 0.0997, 0.0925, 0.0902, 0.1102,\n","         0.1140],\n","        [0.1056, 0.0917, 0.1010, 0.0974, 0.1100, 0.0948, 0.0899, 0.0895, 0.1056,\n","         0.1145],\n","        [0.1063, 0.0899, 0.0932, 0.0911, 0.1099, 0.1030, 0.0864, 0.0915, 0.1085,\n","         0.1201],\n","        [0.1069, 0.0877, 0.0958, 0.0905, 0.1006, 0.1014, 0.0944, 0.0892, 0.1182,\n","         0.1154],\n","        [0.1046, 0.0851, 0.0996, 0.1005, 0.1075, 0.1009, 0.0913, 0.0935, 0.1086,\n","         0.1084],\n","        [0.1083, 0.0847, 0.0921, 0.0969, 0.1096, 0.0990, 0.0891, 0.0977, 0.1097,\n","         0.1130],\n","        [0.1064, 0.0859, 0.0995, 0.0983, 0.1068, 0.0971, 0.0893, 0.0934, 0.1136,\n","         0.1097],\n","        [0.0999, 0.0862, 0.0930, 0.0972, 0.1145, 0.1007, 0.0929, 0.0945, 0.1075,\n","         0.1136],\n","        [0.1054, 0.0904, 0.1065, 0.1014, 0.1072, 0.0977, 0.0938, 0.0821, 0.1078,\n","         0.1076],\n","        [0.1036, 0.0979, 0.1003, 0.0939, 0.1119, 0.1035, 0.0898, 0.0843, 0.1052,\n","         0.1095],\n","        [0.1046, 0.0910, 0.1019, 0.0987, 0.1060, 0.0957, 0.0924, 0.0932, 0.1064,\n","         0.1101],\n","        [0.1119, 0.0886, 0.0978, 0.0886, 0.1037, 0.1004, 0.0864, 0.0920, 0.1108,\n","         0.1198],\n","        [0.1030, 0.0887, 0.1028, 0.0970, 0.1056, 0.1018, 0.0893, 0.0920, 0.1095,\n","         0.1102],\n","        [0.1020, 0.0879, 0.1008, 0.0947, 0.1064, 0.1018, 0.0946, 0.0916, 0.1098,\n","         0.1105],\n","        [0.1104, 0.0865, 0.1045, 0.0981, 0.1080, 0.0963, 0.0920, 0.0864, 0.1048,\n","         0.1129],\n","        [0.1071, 0.0835, 0.1046, 0.0920, 0.1040, 0.1004, 0.0925, 0.0868, 0.1147,\n","         0.1144],\n","        [0.1088, 0.0870, 0.0928, 0.0905, 0.1092, 0.1032, 0.0900, 0.0917, 0.1093,\n","         0.1174],\n","        [0.1089, 0.0893, 0.0968, 0.0951, 0.1085, 0.1005, 0.0873, 0.0892, 0.1113,\n","         0.1131],\n","        [0.1065, 0.0862, 0.1044, 0.0992, 0.1103, 0.1021, 0.0913, 0.0876, 0.1023,\n","         0.1102],\n","        [0.0982, 0.0891, 0.1065, 0.1001, 0.1043, 0.0981, 0.0942, 0.0907, 0.1091,\n","         0.1096],\n","        [0.1077, 0.0815, 0.0992, 0.0991, 0.1108, 0.1018, 0.0919, 0.0883, 0.1093,\n","         0.1104],\n","        [0.0999, 0.0895, 0.1030, 0.0992, 0.1094, 0.0956, 0.0900, 0.0920, 0.1123,\n","         0.1091],\n","        [0.1023, 0.0889, 0.0953, 0.0964, 0.1088, 0.0995, 0.0882, 0.0975, 0.1098,\n","         0.1134],\n","        [0.1024, 0.0877, 0.0950, 0.0977, 0.1093, 0.1025, 0.0901, 0.0939, 0.1084,\n","         0.1129],\n","        [0.1070, 0.0920, 0.0961, 0.0895, 0.0955, 0.0944, 0.0994, 0.0914, 0.1171,\n","         0.1176],\n","        [0.1022, 0.0867, 0.1052, 0.0912, 0.1087, 0.1017, 0.0947, 0.0873, 0.1061,\n","         0.1161],\n","        [0.1071, 0.0871, 0.0995, 0.0975, 0.1050, 0.0987, 0.0892, 0.0891, 0.1157,\n","         0.1110],\n","        [0.1073, 0.0911, 0.0934, 0.0829, 0.1006, 0.0936, 0.0971, 0.0958, 0.1191,\n","         0.1191],\n","        [0.1084, 0.0955, 0.0989, 0.0888, 0.1001, 0.1001, 0.0922, 0.0887, 0.1175,\n","         0.1098],\n","        [0.1068, 0.0897, 0.1022, 0.0958, 0.1118, 0.0947, 0.0842, 0.0902, 0.1088,\n","         0.1159],\n","        [0.1061, 0.0844, 0.1043, 0.0940, 0.1066, 0.1068, 0.0915, 0.0886, 0.1093,\n","         0.1084],\n","        [0.1088, 0.0879, 0.0954, 0.0965, 0.1057, 0.1001, 0.0893, 0.0898, 0.1160,\n","         0.1105],\n","        [0.1062, 0.0898, 0.0950, 0.0921, 0.1110, 0.0994, 0.0873, 0.0936, 0.1107,\n","         0.1149],\n","        [0.1019, 0.0914, 0.0933, 0.0993, 0.1087, 0.0989, 0.0916, 0.0911, 0.1097,\n","         0.1141],\n","        [0.1038, 0.0935, 0.0953, 0.0931, 0.1038, 0.1029, 0.0905, 0.0895, 0.1117,\n","         0.1159],\n","        [0.1042, 0.0879, 0.0972, 0.0942, 0.1077, 0.1038, 0.0906, 0.0914, 0.1125,\n","         0.1105],\n","        [0.1075, 0.0831, 0.1107, 0.0941, 0.1082, 0.1044, 0.0929, 0.0832, 0.1077,\n","         0.1083],\n","        [0.1075, 0.0851, 0.0949, 0.0948, 0.1115, 0.1015, 0.0904, 0.0957, 0.1071,\n","         0.1114],\n","        [0.1056, 0.0873, 0.0983, 0.0952, 0.1024, 0.0986, 0.0907, 0.0911, 0.1171,\n","         0.1137],\n","        [0.1090, 0.0899, 0.1009, 0.0898, 0.1092, 0.1031, 0.0890, 0.0847, 0.1071,\n","         0.1173]], grad_fn=<SoftmaxBackward>)"]},"metadata":{"tags":[]},"execution_count":34}]},{"metadata":{"id":"Puht74jcFn8K","colab_type":"text"},"cell_type":"markdown","source":["NOW, LET CALCULAE CROSS ENTROPY LOSS. WE WILL USE NN.SEQUENTIAL FOR FASTER MODEL BUILDING"]},{"metadata":{"id":"zZ1RaP1aZjZb","colab_type":"code","outputId":"24424aa1-6e9e-4933-9ed7-2f7fe0411995","executionInfo":{"status":"ok","timestamp":1543338768101,"user_tz":-60,"elapsed":115124,"user":{"displayName":"Kenechi Dukor","photoUrl":"","userId":"13097459652657710283"}},"colab":{"base_uri":"https://localhost:8080/","height":105}},"cell_type":"code","source":["# we will be using the mnist dataset. I think the dataset is in pytorch\n","#this cell downloads and runs the dataset\n","!pip install torchvision\n","\n","#!pip install torchvision\n","import torch\n","from torch import nn\n","from torchvision import datasets, transforms\n","import torchvision.transforms as transforms\n","\n","#define a tranform to normiailze the data\n","# transform = transforms.Compose([tranforms.ToTensor(),\n","#                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n","#                                ])\n","import torchvision.transforms as transforms\n","\n","transform = transforms.Compose(\n","    [transforms.ToTensor(),\n","     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n","\n","# download and load the mnist dataset\n","trainset = datasets.MNIST('MNIST_data/', download=True, train=True, transform=transform)\n","trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.2.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.14.6)\n","Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchvision) (0.4.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.11.0)\n","Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (5.3.0)\n"],"name":"stdout"}]},{"metadata":{"id":"9bs6Zg3iIGpb","colab_type":"code","outputId":"8863d884-cf53-4b06-c19f-b6fe3ef78062","executionInfo":{"status":"ok","timestamp":1543338768103,"user_tz":-60,"elapsed":115115,"user":{"displayName":"Kenechi Dukor","photoUrl":"","userId":"13097459652657710283"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["#building the feedforward network\n","model = nn.Sequential(nn.Linear(784, 128),\n","                     nn.ReLU(),\n","                     nn.Linear(128, 64),\n","                     nn.ReLU(),\n","                     nn.Linear(64, 10))\n","\n","# defining the loss\n","criterion = nn.CrossEntropyLoss()\n","\n","#Get the data\n","images, labels = next(iter(trainloader))\n","\n","#flatten images\n","images = images.view(images.shape[0], -1) # or images.view(image.shape[0], 784)\n","\n","# forward pass, get our logits\n","logit = model(images)\n","\n","loss = criterion(logit, labels)\n","\n","print(loss) # this is the cross-entropy loss"],"execution_count":0,"outputs":[{"output_type":"stream","text":["tensor(2.3054, grad_fn=<NllLossBackward>)\n"],"name":"stdout"}]},{"metadata":{"id":"_A97xNJIhUS3","colab_type":"code","outputId":"aa674dcc-29cd-485d-a09e-23f35b5a5bc9","executionInfo":{"status":"ok","timestamp":1543338768104,"user_tz":-60,"elapsed":115103,"user":{"displayName":"Kenechi Dukor","photoUrl":"","userId":"13097459652657710283"}},"colab":{"base_uri":"https://localhost:8080/","height":190}},"cell_type":"code","source":["#now, lets estimate the actual probability by the taking the exponent torch.exp(loss)\n","######confirm if this is correct\n","#building the feedforward network\n","model = nn.Sequential(nn.Linear(784, 128),\n","                     nn.ReLU(),\n","                     nn.Linear(128, 64),\n","                     nn.ReLU(),\n","                     nn.Linear(64, 10))\n","                     \n","\n","\n","#Get the data\n","images, labels = next(iter(trainloader))\n","\n","#flatten images\n","images = images.view(images.shape[0], -1) # or images.view(image.shape[0], 784)\n","\n","# forward pass, get our logits\n","logit = model(images)\n","\n","print(torch.sum(torch.exp(logit), dim=1)) "],"execution_count":0,"outputs":[{"output_type":"stream","text":["tensor([10.0910, 10.1556, 10.2580, 10.0344, 10.1820, 10.4994, 10.4959, 10.1678,\n","        10.3701, 10.2183, 10.3252, 10.2085, 10.0436, 10.4166, 10.1223, 10.3653,\n","        10.1525, 10.2601, 10.1413, 10.3403, 10.3168, 10.3659, 10.2248, 10.3532,\n","        10.1959, 10.5928, 10.0587, 10.3399, 10.1809, 10.2990, 10.2109, 10.1128,\n","        10.4850, 10.5144, 10.3668, 10.1261, 10.1131, 10.1703, 10.2077, 10.0374,\n","        10.3119, 10.1669, 10.4046, 10.0540, 10.2979, 10.2392,  9.9482, 10.3441,\n","        10.3169, 10.0473, 10.2988, 10.3500, 10.1274, 10.4191, 10.2568, 10.2214,\n","        10.3306, 10.3166, 10.3917, 10.2984, 10.1317, 10.1524, 10.1873, 10.1339],\n","       grad_fn=<SumBackward1>)\n"],"name":"stdout"}]},{"metadata":{"id":"zk__vXXEh54Q","colab_type":"code","outputId":"9f551b27-a089-457c-b268-dea3d4edb046","executionInfo":{"status":"ok","timestamp":1543338768106,"user_tz":-60,"elapsed":115092,"user":{"displayName":"Kenechi Dukor","photoUrl":"","userId":"13097459652657710283"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["# Now, according to the tutorial, we can use LogSoftmax as follows to get the log and softmax\n","\n","#building the feedforward network\n","model = nn.Sequential(nn.Linear(784, 128),\n","                     nn.ReLU(),\n","                     nn.Linear(128, 64),\n","                     nn.ReLU(),\n","                     nn.Linear(64, 10),\n","                     nn.LogSoftmax(dim=1))\n","                    \n","\n","# defining the loss\n","criterion = nn.NLLLoss() #negative log-likelihood loss\n","\n","#Get the data\n","images, labels = next(iter(trainloader))\n","\n","#flatten images\n","images = images.view(images.shape[0], -1) # or images.view(image.shape[0], 784)\n","\n","# forward pass, get our logits\n","log_probability = model(images)\n","\n","loss = criterion(log_probability, labels)\n","\n","print(loss) # this is the cross-entropy loss"],"execution_count":0,"outputs":[{"output_type":"stream","text":["tensor(2.2965, grad_fn=<NllLossBackward>)\n"],"name":"stdout"}]},{"metadata":{"id":"mXw8Ef28k59A","colab_type":"text"},"cell_type":"markdown","source":["AUTOGRAD (AUTO GRADIENT)"]},{"metadata":{"id":"Q6dGP2IYk5Nh","colab_type":"code","outputId":"fbf618db-5809-4415-f3bb-be559fe0288c","executionInfo":{"status":"ok","timestamp":1543338768108,"user_tz":-60,"elapsed":115081,"user":{"displayName":"Kenechi Dukor","photoUrl":"","userId":"13097459652657710283"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"cell_type":"code","source":["#Prove this mathematically\n","#the function .backward() tracks the gradient backward. for example\n","\n","x = torch.randn((2,2), requires_grad = True)\n","print(x)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["tensor([[-0.4915,  0.0679],\n","        [ 1.0011, -0.0203]], requires_grad=True)\n"],"name":"stdout"}]},{"metadata":{"id":"3tZF-_7eoxbP","colab_type":"code","outputId":"8ae9aeaa-7f66-4e48-ad23-a178d14ee57d","executionInfo":{"status":"ok","timestamp":1543338768117,"user_tz":-60,"elapsed":115078,"user":{"displayName":"Kenechi Dukor","photoUrl":"","userId":"13097459652657710283"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"cell_type":"code","source":["y = x**2\n","print(y)\n","print(y.grad_fn)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["tensor([[0.2416, 0.0046],\n","        [1.0022, 0.0004]], grad_fn=<PowBackward0>)\n","<PowBackward0 object at 0x7f34bb0825c0>\n"],"name":"stdout"}]},{"metadata":{"id":"l9jUSjeho23X","colab_type":"code","outputId":"cd5ce739-3ad0-4439-d8d4-0c94cc8e3636","executionInfo":{"status":"ok","timestamp":1543338768119,"user_tz":-60,"elapsed":115066,"user":{"displayName":"Kenechi Dukor","photoUrl":"","userId":"13097459652657710283"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["#z = torch.mean(y) #or\n","z = y.mean()\n","print(z)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["tensor(0.3122, grad_fn=<MeanBackward1>)\n"],"name":"stdout"}]},{"metadata":{"id":"KcAauTUypqSs","colab_type":"code","outputId":"9ba14e04-227a-41b3-b9e5-a419e49ce631","executionInfo":{"status":"ok","timestamp":1543338768121,"user_tz":-60,"elapsed":115047,"user":{"displayName":"Kenechi Dukor","photoUrl":"","userId":"13097459652657710283"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["print(x.grad)# it is none because we havent calculated the gradient"],"execution_count":0,"outputs":[{"output_type":"stream","text":["None\n"],"name":"stdout"}]},{"metadata":{"id":"yQ-kpnWrpLBY","colab_type":"code","outputId":"5912be51-242a-4811-d40e-8c3a06f0e891","executionInfo":{"status":"ok","timestamp":1543338768122,"user_tz":-60,"elapsed":115035,"user":{"displayName":"Kenechi Dukor","photoUrl":"","userId":"13097459652657710283"}},"colab":{"base_uri":"https://localhost:8080/","height":85}},"cell_type":"code","source":["# now, lets find the gradient backward. ie dz/dx\n","z.backward()\n","print(x.grad)\n","print(x/2)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["tensor([[-0.2458,  0.0340],\n","        [ 0.5005, -0.0102]])\n","tensor([[-0.2458,  0.0340],\n","        [ 0.5005, -0.0102]], grad_fn=<DivBackward0>)\n"],"name":"stdout"}]},{"metadata":{"id":"W7KRWnYXqK1P","colab_type":"text"},"cell_type":"markdown","source":["LOSS AND AUTOGRAD TOGETHER"]},{"metadata":{"id":"hKXXK7gQqP6k","colab_type":"code","colab":{}},"cell_type":"code","source":["#Lets try this backwar pass with our previous network\n","\n","#building the feedforward network\n","model = nn.Sequential(nn.Linear(784, 128),\n","                     nn.ReLU(),\n","                     nn.Linear(128, 64),\n","                     nn.ReLU(),\n","                     nn.Linear(64, 10),\n","                     nn.LogSoftmax(dim=1))\n","                    \n","\n","# defining the loss\n","criterion = nn.NLLLoss() #negative log-likelihood loss\n","\n","#Get the data\n","images, labels = next(iter(trainloader))\n","\n","#flatten images\n","images = images.view(images.shape[0], -1) # or images.view(image.shape[0], 784)\n","\n","# forward pass, get our logits\n","log_probability = model(images)\n","\n","loss = criterion(log_probability, labels)\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ud9f1OgCq2cY","colab_type":"code","outputId":"7630a6ab-15a1-4999-b950-3178c1d3bc2a","executionInfo":{"status":"ok","timestamp":1543338768386,"user_tz":-60,"elapsed":115280,"user":{"displayName":"Kenechi Dukor","photoUrl":"","userId":"13097459652657710283"}},"colab":{"base_uri":"https://localhost:8080/","height":187}},"cell_type":"code","source":["print(\"Before backward pass: \\n\", model[0].weight.grad) #model[0] refers to the first layer (nn.Linear(784, 128))\n","\n","loss.backward()\n","\n","print(\"After backward pass: \\n\", model[0].weight.grad)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Before backward pass: \n"," None\n","After backward pass: \n"," tensor([[-0.0038, -0.0038, -0.0038,  ..., -0.0038, -0.0038, -0.0038],\n","        [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n","        [ 0.0002,  0.0002,  0.0002,  ...,  0.0002,  0.0002,  0.0002],\n","        ...,\n","        [-0.0071, -0.0071, -0.0071,  ..., -0.0071, -0.0071, -0.0071],\n","        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","        [ 0.0004,  0.0004,  0.0004,  ...,  0.0004,  0.0004,  0.0004]])\n"],"name":"stdout"}]},{"metadata":{"id":"9QCv5-2Arju0","colab_type":"text"},"cell_type":"markdown","source":["But how to we use these gradients to optimise our weights"]},{"metadata":{"id":"_8ihuNMarsA1","colab_type":"code","colab":{}},"cell_type":"code","source":["# we will use pytorch optimisers. we could stocastic gradient descent (optim.SGD)\n","# for example\n","\n","from torch import optim\n","\n","optimizer = optim.SGD(model.parameters(), lr = 0.01) #where lr is the learning rate"],"execution_count":0,"outputs":[]},{"metadata":{"id":"j-TAwT2mtVBh","colab_type":"code","outputId":"ccb367ad-a5b2-4b93-b924-278fd9b8b6d0","executionInfo":{"status":"ok","timestamp":1543338768392,"user_tz":-60,"elapsed":115269,"user":{"displayName":"Kenechi Dukor","photoUrl":"","userId":"13097459652657710283"}},"colab":{"base_uri":"https://localhost:8080/","height":309}},"cell_type":"code","source":["#now, lets train one step\n","print ('inital weight = ', model[0].weight)\n","\n","images, labels = next(iter(trainloader))\n","images.resize_(64, 784) #flatten the image\n","\n","#Clear the previously generated gradient because gradients are accumulated\n","optimizer.zero_grad()\n","\n","#forward pass then backward pass then update weight\n","output = model.forward(images)\n","loss = criterion(output, labels)\n","\n","loss.backward()\n","\n","print(\"Gradient = \", model[0].weight.grad)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["inital weight =  Parameter containing:\n","tensor([[ 0.0071, -0.0196,  0.0190,  ..., -0.0304,  0.0179,  0.0320],\n","        [-0.0304,  0.0305, -0.0137,  ...,  0.0113,  0.0259, -0.0251],\n","        [-0.0342, -0.0325, -0.0129,  ...,  0.0082,  0.0295, -0.0347],\n","        ...,\n","        [-0.0094,  0.0345, -0.0274,  ..., -0.0133,  0.0006,  0.0330],\n","        [ 0.0006,  0.0356, -0.0269,  ...,  0.0109,  0.0053, -0.0206],\n","        [-0.0100,  0.0077, -0.0280,  ...,  0.0123,  0.0257, -0.0078]],\n","       requires_grad=True)\n","Gradient =  tensor([[ 0.0021,  0.0021,  0.0021,  ...,  0.0021,  0.0021,  0.0021],\n","        [-0.0008, -0.0008, -0.0008,  ..., -0.0008, -0.0008, -0.0008],\n","        [-0.0002, -0.0002, -0.0002,  ..., -0.0002, -0.0002, -0.0002],\n","        ...,\n","        [-0.0005, -0.0005, -0.0005,  ..., -0.0005, -0.0005, -0.0005],\n","        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","        [ 0.0010,  0.0010,  0.0010,  ...,  0.0010,  0.0010,  0.0010]])\n"],"name":"stdout"}]},{"metadata":{"id":"s4N_A1jwvlqz","colab_type":"code","outputId":"dcce0025-b6de-4926-a49d-d8415da7d83b","executionInfo":{"status":"ok","timestamp":1543338768393,"user_tz":-60,"elapsed":115251,"user":{"displayName":"Kenechi Dukor","photoUrl":"","userId":"13097459652657710283"}},"colab":{"base_uri":"https://localhost:8080/","height":170}},"cell_type":"code","source":["#take an update step to update the weights\n","optimizer.step()\n","print(\"Update weights = \", model[0].weight)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Update weights =  Parameter containing:\n","tensor([[ 0.0071, -0.0196,  0.0190,  ..., -0.0305,  0.0178,  0.0320],\n","        [-0.0304,  0.0305, -0.0137,  ...,  0.0113,  0.0259, -0.0251],\n","        [-0.0342, -0.0325, -0.0129,  ...,  0.0082,  0.0296, -0.0347],\n","        ...,\n","        [-0.0093,  0.0345, -0.0274,  ..., -0.0133,  0.0006,  0.0330],\n","        [ 0.0006,  0.0356, -0.0269,  ...,  0.0109,  0.0053, -0.0206],\n","        [-0.0100,  0.0077, -0.0280,  ...,  0.0123,  0.0257, -0.0078]],\n","       requires_grad=True)\n"],"name":"stdout"}]},{"metadata":{"id":"PQ_ysynsxXmA","colab_type":"text"},"cell_type":"markdown","source":["CLASS WORK LETS TRAIN WITH MULTPLE STEPS"]},{"metadata":{"id":"8c7Uap3SxeYA","colab_type":"code","outputId":"0fd104a8-5605-4086-d0c6-66156f3eeb2f","executionInfo":{"status":"ok","timestamp":1543338808325,"user_tz":-60,"elapsed":155166,"user":{"displayName":"Kenechi Dukor","photoUrl":"","userId":"13097459652657710283"}},"colab":{"base_uri":"https://localhost:8080/","height":102}},"cell_type":"code","source":["#building the feedforward network\n","model = nn.Sequential(nn.Linear(784, 128),\n","                     nn.ReLU(),\n","                     nn.Linear(128, 64),\n","                     nn.ReLU(),\n","                     nn.Linear(64, 10),\n","                     nn.LogSoftmax(dim=1))\n","                    \n","\n","# defining the loss\n","criterion = nn.NLLLoss() #negative log-likelihood loss\n","optimizer = optim.SGD(model.parameters(), lr = 0.03) #where lr is the learning rate\n","epochs = 5\n","\n","for e in range(epochs):\n","  running_loss = 0\n","  for images, labels in trainloader:\n","    \n","    #flatten images\n","    images = images.view(images.shape[0], -1) # or images.view(image.shape[0], 784)\n","    \n","    optimizer.zero_grad()\n","    \n","    # forward pass, get our logits\n","    log_probability = model(images) #or output = model.forward(images)\n","\n","    \n","    loss = criterion(log_probability, labels)\n","    \n","    loss.backward()\n","    \n","    optimizer.step()\n","    \n","    running_loss += loss.item()\n","  else:\n","    print(f\"Training Loss: {running_loss/len(trainloader)}\")\n","    \n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Training Loss: 0.6069350083118309\n","Training Loss: 0.2777405214954668\n","Training Loss: 0.21222881767859084\n","Training Loss: 0.17022348124224113\n","Training Loss: 0.14247178787123294\n"],"name":"stdout"}]},{"metadata":{"id":"ETQc2s070bc5","colab_type":"code","outputId":"bdfa2388-180a-4744-d0fa-2a3763a114e4","executionInfo":{"status":"error","timestamp":1543338809506,"user_tz":-60,"elapsed":156336,"user":{"displayName":"Kenechi Dukor","photoUrl":"","userId":"13097459652657710283"}},"colab":{"base_uri":"https://localhost:8080/","height":387}},"cell_type":"code","source":["# now, lets pass in an image to see what the network is predicting\n","%matplotlib inline\n","#!pip install helper\n","\n","#from google.colab import files\n","#src = list(files.upload().values())[0]\n","#open('helper.py','wb').write(src)\n","\n","#!wget \"https://github.com/udacity/deep-learning-v2-pytorch/blob/master/intro-to-pytorch/helper.py\"\n","\n","import helpers\n","import torch.nn.functional as F\n","\n","images, labels = next(iter(trainloader))\n","\n","img = images[0].view(1, 784)\n","\n","#lets turn off the gradients\n","\n","with torch.no_grad():\n","  logits = model.forward(img)\n","\n","#Output of the network are logits, need to take softmax of probabilities\n","\n","ps = F.softmax(logits, dim=1)\n","\n","#ps = nn.LogSoftmax(logits)\n","\n","helpers.view_classify(img.view(1, 28, 28), ps)"],"execution_count":0,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-50-3e7d25932dee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m#!wget \"https://github.com/udacity/deep-learning-v2-pytorch/blob/master/intro-to-pytorch/helper.py\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mhelpers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'helpers'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"]}]}]}